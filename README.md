âœ‹ Sign Language Interpreter â€“ Starter Repository

A minimal, ready-to-run Sign Language Interpreter starter project using hand landmarks, a lightweight MLP model, and optional MediaPipe Hands for realtime detection.
This repo is built to run without errors even if MediaPipe is not installed â€” it automatically falls back to synthetic/random data for training & demo.

Perfect for beginners, students, prototypes, and anyone who wants a clean base project to extend into a full gesture-recognition system.

#Project Structure

project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ capture.py          # Capture hand landmarks for any label
â”‚   â”œâ”€â”€ train.py            # Train a simple MLP classifier
â”‚   â”œâ”€â”€ realtime.py         # Realtime gesture detection demo
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ mlp.py          # Simple MLP model builder
â”‚   â”œâ”€â”€ utils.py            # Helper functions (saving/loading encoder, etc.)
â”‚   â””â”€â”€ voice.py            # Text-to-Speech using pyttsx3 (optional)
â”‚
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ best_model.h5       # Saved model after training (auto-created)
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸš€ Features

âœ” Works even if MediaPipe is NOT installed. (recommended to use MediaPipe)
âœ” Automatically switches to synthetic/fallback data
âœ” Lightweight MLP classifier for fast training
âœ” Realtime demo with optional text-to-speech
âœ” Clean modular structure
âœ” Easy to extend to CNN, LSTM, or full TF models

ğŸ Getting Started

1ï¸âƒ£ Create Virtual Environment & Install Requirements
*windows

	 python -m venv venv
     venv\Scripts\activate
     pip install -r requirements.txt

*Linux/macOS

	 python -m venv venv
 	source venv/bin/activate
 	pip install -r requirements.txt
	python -m textblob.download_corpora

2ï¸âƒ£ Capture Gesture Samples
	
Use this script to collect your own gesture dataset such as:
hello, yes, no, stop, ok, thanks, etc.
Example: 

	python -m src.capture --label hello --samples 200
General format:

	python -m src.capture.py --label <label_name> --samples <count>

The more samples you record, the better the model performs.

Captured data is automatically saved under:

data/<label_name>/

ğŸ§  3ï¸âƒ£ Train the Model

After collecting several gestures, train your classifier:

      python -m src.train --epochs 15 --augment-times 1


This generates:

experiments/best_model.h5
experiments/label_encoder.pkl

## OPTIONAL:-- Check the accuracy of the trained model....
	python -m src.evaluate


4ï¸âƒ£ Run Realtime Interpreter

To run realtime gesture recognition:

      python -m src.realtime --conf-thresh 0.6 --window 8


Behavior:

If MediaPipe Hands is installed â†’ realtime webcam detection

If NOT â†’ simulated detection demo (no errors)


ğŸ”Š Optional: Enable Voice Output

voice.py uses pyttsx3 to convert predictions into speech.

If installed:

       pip install pyttsx3




